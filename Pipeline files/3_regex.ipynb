{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3d378-f28a-46b7-8782-2b82712b7c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern is interpreted as a regular expression')\n",
    "bucket = \"\"\n",
    "filter_out = []\n",
    "#Name should be without '.csv'\n",
    "export_name = \"data/regex_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f44da-e9a3-48c3-ad42-6c0831a33bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assertion file\n",
    "n3 = pd.read_csv(f\"s3://{bucket}/data/processed_notes_assertions.csv\")\n",
    "#token file\n",
    "expd = pd.read_csv(f\"s3://{bucket}/data/processed_notes_tokens.csv\")\n",
    "notes = pd.read_csv(f\"s3://{bucket}/data/note_set_cleaned.csv\")\n",
    "notes = notes.drop_duplicates(subset=[\"notes_id\"])\n",
    "expd.notes_id = expd.notes_id.astype(str)\n",
    "n3.notes_id = n3.notes_id.astype(str)\n",
    "\n",
    "expd = expd.drop_duplicates(subset=[\"notes_id\", \"token_start\", \"token_end\"])\n",
    "expd = expd.dropna(subset=\"tokens\")\n",
    "expd = expd.reset_index(drop=True)\n",
    "\n",
    "notes = notes[notes.notes_id.isin(expd.notes_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ce1ca-1326-4b91-9f11-399190b058d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "#takes: list of regex searches for terms of interest, context chars(int) default=75\n",
    "#returns: table with token[list], classification[list], any overlapping assertion chunk & status, \n",
    "#text +- context characters from token\n",
    "def gen_table_text(search, disallow, chars=75):\n",
    "    #search for terms\n",
    "    found = []\n",
    "    for term in search.keys():\n",
    "        searched = notes[notes.cleaned_notes.str.contains(search[term])]\n",
    "        searched[\"concept\"] = term\n",
    "         #solution courtesy of https://stackoverflow.com/questions/26658213        \n",
    "        searched['regex_output_tuple'] = searched['cleaned_notes'].apply(get_regex_output,\n",
    "                                                                         args=(search[term],))\n",
    "\n",
    "        #convert the tuple into separate columns\n",
    "        columns_from_regex_output = ['start','end']      \n",
    "        for n, col in enumerate(columns_from_regex_output):\n",
    "            searched[col] = searched['regex_output_tuple'].apply(lambda x: x[n])\n",
    "        #delete the unnecessary column\n",
    "        searched = searched.drop('regex_output_tuple', axis=1)\n",
    "        found.append(searched)\n",
    "    found = pd.concat(found).reset_index(drop=True)\n",
    "    \n",
    "    #knit together matching tokens and assertion chunks\n",
    "    toks = []\n",
    "    found.apply(get_tokens, axis=1, args=(toks,))\n",
    "    if len(toks) > 0:\n",
    "        toks = pd.concat(toks).reset_index(drop=True)\n",
    "\n",
    "        #drop duplicates\n",
    "        toks = toks.drop_duplicates(subset=[\"token_start\", \"notes_id\"]).reset_index(drop=True)\n",
    "\n",
    "        #lastly, get context text\n",
    "        toks[\"text\"] = toks.apply(get_text, axis=1, args=(chars,))\n",
    "\n",
    "    else:\n",
    "        toks = pd.DataFrame()\n",
    "                \n",
    "    return toks\n",
    "\n",
    "#helper function to obtain regex output (start, stop) as a tuple\n",
    "def get_regex_output(row, term):\n",
    "    find = re.finditer(term, row)\n",
    "    starts = []\n",
    "    ends = []\n",
    "    for i in find:\n",
    "        starts.append(i.start())\n",
    "        ends.append(i.end())\n",
    "    return(starts, ends)\n",
    "    \n",
    "#helper function for gen_table_text\n",
    "#takes a row from pd with extracted start/stops, new df\n",
    "#adds to df list of tokens within [start, stop]\n",
    "def get_tokens(row, df):\n",
    "    starts = row.start\n",
    "    ends = row.end\n",
    "    tok_locs = list(zip(starts, ends))\n",
    "    note = expd.loc[expd.notes_id.isin([row.notes_id])]\n",
    "    forbid_start = [\"and\", \"to\", \"of\", \"or\", \"is\", \"a\", \"for\", \"s\", \"an\", \"if\", \"be\", \"as\",\n",
    "                   \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",\n",
    "                    \",\", \":\", \";\", \".\"]\n",
    "    tok_list = []\n",
    "    notes_id_list = []\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    clinical_list = []\n",
    "    wips_list = []\n",
    "    bert_list = []\n",
    "    concept_list = []\n",
    "    for tok in tok_locs:\n",
    "        start = tok[0]\n",
    "        end = tok[1]\n",
    "        new_toks = []\n",
    "        new_clinical = []\n",
    "        new_wips = []\n",
    "        new_bert = []\n",
    "        #get row of start tokens\n",
    "        #sometimes numbers are off\n",
    "        start_row = note[note.token_start.isin([start])]\n",
    "        start_row = start_row[~start_row.tokens.isin(forbid_start)]\n",
    "        if len(start_row) < 1:\n",
    "            start_row = note[(note.token_start >= start-5) & (note.token_start <= start+5)]\n",
    "            start_row = start_row[~start_row.tokens.isin(forbid_start)]\n",
    "            if len(start_row) < 1:\n",
    "                #attempt to find target text in rows\n",
    "                target = row.cleaned_notes[start:end]\n",
    "                start_row = note[(note.token_start >= start-25) & (note.token_start <= start+15)]\n",
    "                start_row = start_row[start_row.tokens.str.contains(target) | \n",
    "                                     start_row.tokens.str.contains(target[-5:-1])]\n",
    "                if len(start_row) < 1:\n",
    "                    start_row = note[(note.token_start >= start-25) & (note.token_start <= start+15)]\n",
    "                    continue\n",
    "        start_row = start_row.iloc[0]\n",
    "        if start_row.token_end+1 < end:\n",
    "            end_row = note[(note.token_end == end) & (note.token_end <= end)]\n",
    "            end_row = end_row[~end_row.tokens.isin(forbid_start)]\n",
    "            if len(end_row) < 1:\n",
    "                end_row = note[(note.token_end >= end-5) & \n",
    "                                        (note.token_end <= end+5)]\n",
    "                if len(end_row) < 1:\n",
    "                    #attempt to find end of target text in rows\n",
    "                    target = row.cleaned_notes[start:end]\n",
    "                    end_row = note[(note.tokens.str.contains(target))|\n",
    "                                           (note.tokens.str.contains(target[-4:]))]\n",
    "                    if len(end_row) < 1:\n",
    "                        continue\n",
    "            end_row = end_row.iloc[0].name\n",
    "        else:\n",
    "            end_row = start_row.name\n",
    "        if end_row < start_row.name:\n",
    "            end_row = start_row.name\n",
    "        elif end_row > start_row.name + 20:\n",
    "            end_row = start_row.name + 1\n",
    "        start_row = start_row.name\n",
    "        end_row += 1\n",
    "        for r in range(start_row, end_row):\n",
    "            new_toks.append(expd.iloc[r].tokens)\n",
    "            new_clinical.append(expd.iloc[r].clinical_ner)\n",
    "            new_wips.append(expd.iloc[r].wip_ner)\n",
    "            new_bert.append(expd.iloc[r].BERT_ner)\n",
    "        tok_list.append(new_toks)\n",
    "        clinical_list.append(new_clinical)\n",
    "        wips_list.append(new_wips)\n",
    "        bert_list.append(new_bert)\n",
    "        notes_id_list.append(row.notes_id)\n",
    "        start_list.append(start)\n",
    "        end_list.append(end)\n",
    "        concept_list.append(row.concept)\n",
    "    if len(tok_list) > 0:\n",
    "        tok_list = pd.DataFrame([tok_list, start_list, end_list, concept_list, clinical_list,\n",
    "                                 wips_list, bert_list, notes_id_list]).transpose()\n",
    "        tok_list.columns = [\"tokens\", \"token_start\", \"token_end\", \"concept\",\n",
    "                            \"clinical_ner\", \"wip_ner\", \"BERT_ner\", \"notes_id\"]\n",
    "        df.append(tok_list)\n",
    "        #now let's get chunks...\n",
    "        tab = []\n",
    "        tok_list.apply(find_chunks, args=(tab,), axis=1)\n",
    "        tab = pd.concat(tab).reset_index(drop=True)\n",
    "        toks = pd.concat([tok_list, tab], axis=1)\n",
    "        tab = []\n",
    "        toks.apply(check_chunks, args=(tab, note), axis=1)\n",
    "        toks = pd.concat(tab, axis=1).transpose().reset_index(drop=True)\n",
    "        \n",
    "        df.append(toks)\n",
    "    \n",
    "    \n",
    "#helper function for gen_table_text\n",
    "#takes df with tokens, returns any overlapping chunks found\n",
    "def find_chunks(row, df):\n",
    "    start = int(row.token_start)\n",
    "    end = int(row.token_end)\n",
    "    chunks = n3.loc[n3.notes_id.isin([row.notes_id])]\n",
    "    chunks = chunks[(chunks.chunk_begin >= start)\\\n",
    "             & (chunks.chunk_begin < end)]\n",
    "    chunks = pd.DataFrame([[list(chunks.chunks),\n",
    "                            list(chunks.entities),\n",
    "                            list(chunks.assertion)]],\n",
    "                          columns=[\"chunks\", \"entities\", \"assertion\"])\n",
    "    df.append(chunks)\n",
    "    \n",
    "#takes a row from our regex'd table + blank list\n",
    "#checks to see if we need to go backwards to get beginning of chunks\n",
    "def check_chunks(row, tab, note):\n",
    "    # note = expd[expd.notes_id.isin([row.notes_id])]\n",
    "    note = note[(note.token_start < row.token_start + 20) &\n",
    "               (note.token_start > row.token_start - 20)]\n",
    "    start = row.token_start\n",
    "    end = row.token_end\n",
    "    \n",
    "    start_row = note[note.tokens.isin([row.tokens[0]])]\n",
    "    #sometimes you get two of the same word close together!\n",
    "    #make sure we get the one we actually wanted!\n",
    "    if len(start_row) > 1:\n",
    "        start_row['dist'] = abs(start_row.token_start - start)\n",
    "        start_row = start_row[start_row.dist.isin([start_row.dist.min()])]\n",
    "        \n",
    "    if len(start_row) > 0:\n",
    "        start_row = start_row.iloc[0]\n",
    "        end_row = start_row.name + (len(row.tokens) - 1)\n",
    "        clin_row = [row.clinical_ner[0], row.wip_ner[0], row.BERT_ner[0]]\n",
    "        ners = get_ner(clin_row)\n",
    "        new_start = start_row.name\n",
    "        if ners == \"i\":\n",
    "            #then we need to go back until we find a 'b'\n",
    "            new_start = start_row.name - 1\n",
    "            cur_start = expd.iloc[new_start]\n",
    "            start_ners = get_ner([cur_start.clinical_ner, cur_start.wip_ner, cur_start.BERT_ner])\n",
    "            if start_ners != \"b\":\n",
    "                while start_ners != \"b\":\n",
    "                    new_start = start_row.name - 1\n",
    "                    cur_start = expd.iloc[new_start]\n",
    "                    start_row = cur_start\n",
    "                    start_ners = get_ner([cur_start.clinical_ner, cur_start.wip_ner, cur_start.BERT_ner])\n",
    "        elif ners == \"o\":\n",
    "            #check if we have an 'i' or 'b' ahead till end row\n",
    "            set_new = False\n",
    "            while new_start < expd.iloc[end_row].name:\n",
    "                new_start += 1\n",
    "                cur_start = expd.iloc[new_start]\n",
    "                start_ners = get_ner([cur_start.clinical_ner, cur_start.wip_ner, cur_start.BERT_ner])\n",
    "                if start_ners != \"o\":\n",
    "                    set_new = True\n",
    "                    break\n",
    "            if set_new != True:\n",
    "                new_start = start_row.name\n",
    "        #and check after as well\n",
    "        cur_end = expd.iloc[end_row]\n",
    "        end_ners = get_ner([cur_end.clinical_ner, cur_end.wip_ner, cur_end.BERT_ner])\n",
    "        new_end = end_row + 1\n",
    "        next_ners = get_ner([expd.iloc[new_end].clinical_ner,\n",
    "                             expd.iloc[new_end].wip_ner,\n",
    "                             expd.iloc[new_end].BERT_ner])\n",
    "        if end_ners == \"i\" or next_ners == \"i\":\n",
    "            if end_ners != \"i\":\n",
    "                end_ners = next_ners\n",
    "                end_row = new_end\n",
    "            while end_ners == \"i\":\n",
    "                new_end = end_row + 1\n",
    "                cur_end = expd.iloc[new_end]\n",
    "                if cur_end.notes_id != row.notes_id:\n",
    "                    new_end = end_row - 1\n",
    "                    break\n",
    "                else:\n",
    "                    end_row = cur_end\n",
    "                    end_ners = get_ner([cur_end.clinical_ner, cur_end.wip_ner, cur_end.BERT_ner])\n",
    "                    end_row = end_row.name\n",
    "        #now to yank off those ending 'o's that snuck in\n",
    "        #first see if we're already too long\n",
    "        if new_end - new_start > 5:\n",
    "            new_end = new_start + 5\n",
    "        if end_ners == \"o\" and ners != \"o\":\n",
    "            while end_ners == \"o\":\n",
    "                new_end = end_row - 1\n",
    "                cur_end = expd.iloc[new_end]\n",
    "                end_row = cur_end\n",
    "                end_ners = get_ner([cur_end.clinical_ner, cur_end.wip_ner, cur_end.BERT_ner])\n",
    "                end_row = end_row.name\n",
    "            new_end = new_end + 1\n",
    "        #TODO probable refinement here on how to chop down excessively long NER strings\n",
    "        if new_end - new_start > 5:\n",
    "            new_end = new_start + 5\n",
    "        if new_start >= new_end:\n",
    "            new_end += 1\n",
    "        whole_df = expd.iloc[new_start:new_end]\n",
    "        if len(whole_df) == 0:\n",
    "            new_end = new_start + 1\n",
    "            while len(expd.iloc[new_start:new_end]) == 0:\n",
    "                new_start -= 1\n",
    "                new_end = new_start + 1\n",
    "            whole_df = expd.iloc[new_start:new_end]\n",
    "        row.token_start = whole_df.iloc[0].token_start\n",
    "        row.token_end = whole_df.iloc[-1].token_end + 1\n",
    "        row.tokens = list(whole_df.tokens)\n",
    "        row.clinical_ner = list(whole_df.clinical_ner)\n",
    "        row.wip_ner = list(whole_df.wip_ner)\n",
    "        row.BERT_ner = list(whole_df.BERT_ner)\n",
    "    tab.append(pd.DataFrame(row))\n",
    "        \n",
    "#helper function for gen_table\n",
    "#takes row and chars (int)\n",
    "#returns the += chars context around the tokens\n",
    "def get_text(row, chars):\n",
    "    n = notes[notes.notes_id == row.notes_id].cleaned_notes.values[0]\n",
    "\n",
    "    start = row.token_start\n",
    "    start -= chars\n",
    "    end = row.token_end# + 1\n",
    "    end += chars\n",
    "    \n",
    "    #rare edge case where token_end didn't get recorded properly\n",
    "    if end < start:\n",
    "        end = row.token_start + chars\n",
    "        for i in row.tokens:\n",
    "            end += len(i)\n",
    "    \n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end > len(n):\n",
    "        end = len(n)\n",
    "        \n",
    "    return n[start:end]\n",
    "\n",
    "#helper function for find_chunks\n",
    "#receives list (clinical, wip, bert)\n",
    "#returns whether one NER starts with 'i'; if not, 'b'; if not, 'o'\n",
    "def get_ner(row):\n",
    "    if row[0][0] == \"B\" or\\\n",
    "       row[1][0] == \"B\" or\\\n",
    "       row[2][0] == \"B\":\n",
    "        if row[0] and row[1] and row[2] != \"B-Gender\":\n",
    "            return(\"b\")\n",
    "        else:\n",
    "            return(\"o\")\n",
    "    elif row[0][0] == \"I\" or\\\n",
    "       row[1][0] == \"I\" or\\\n",
    "       row[2][0] == \"I\":\n",
    "        return(\"i\")\n",
    "    else:\n",
    "        return(\"o\")\n",
    "\n",
    "#helper function for get_chunks\n",
    "#returns df with tokens with disallowed text (puncutation) removed\n",
    "def clean_tokens(whole_df, disallow):\n",
    "    tokens = list(whole_df.tokens)\n",
    "    clinical = list(whole_df.clinical_ner)\n",
    "    wip = list(whole_df.wip_ner)\n",
    "    bert = list(whole_df.BERT_ner)\n",
    "    #currently we have decided not to do any of this\n",
    "    # remove = []\n",
    "    # for i in range(len(tokens)):\n",
    "    #     if tokens[i] in disallow:\n",
    "    #         remove.append(i)\n",
    "    # for i in range(len(remove)):\n",
    "    #     rm = remove[i]\n",
    "    #     rm -= i\n",
    "    #     # print(remove, rm, tokens[rm])\n",
    "    #     del tokens[rm]\n",
    "    #     del clinical[rm]\n",
    "    #     del wip[rm]\n",
    "    #     del bert[rm]\n",
    "    #     # print(tokens)\n",
    "    res = pd.DataFrame([[tokens,\n",
    "                        whole_df.iloc[0].token_start,\n",
    "                        whole_df.iloc[-1].token_end,\n",
    "                        clinical, wip, bert]],\n",
    "                        columns=[\"tokens\", \"token_start\", \"token_end\", \"clinical_ner\", \"wip_ner\", \"BERT_ner\"])\n",
    "    return res\n",
    "\n",
    "#Current version of our regex assertions\n",
    "def newer_regex(row, start=60, end=7):\n",
    "    nots1 = re.search('(\\\\bnot?\\\\b|\\\\bden(y(ing)?|ies|ied)|\\\\bneg\\\\w*|never|resolved|absent)\\\\W+(?:\\\\w+\\\\W+){0,2}?',\n",
    "                  row.text[100-start:100],\n",
    "                 flags=re.I)\n",
    "    if len(row.text[100-start:100].split(',')) > 2:\n",
    "        nots1 = re.search('(\\\\bnot?\\\\b|\\\\bden(y(ing)?|ies|ied)|\\\\bneg\\\\w*|never|resolved|absent)\\\\W+(?:\\\\w+\\\\W+){0,2}?',\n",
    "              row.text[0:100],\n",
    "             flags=re.I)\n",
    "    nots2 = re.search('(\\\\bnot?\\\\b|\\\\bden(y|ies|ied)|\\\\bneg\\\\w*|never|resolved|none|0|absent)\\\\W+(?:\\\\w+\\\\W+){0,2}?',\n",
    "                    row.text[100+len(row.tokens)-3:100+len(row.tokens)+end],\n",
    "                     flags=re.I)\n",
    "    nots3 = re.search('\\\\[\\\\]|\\\\(-|--',\n",
    "                      row.text[85:100],\n",
    "                      flags=re.I)\n",
    "    nots4 = re.search('if you have|as needed|go to ED if|if other symptoms|(once|twice) (daily|a day)|\\\n",
    "tablets at|minimum of 24 hours|recurrence of',\n",
    "                      row.text[40:100],\n",
    "                      flags=re.I)\n",
    "    nots5 = re.search('in (his|her|their)*\\s*(mother|father|grandmother|grandfather|brother|sister)|\\\n",
    "maternal|paternal|family history|past medical history|PMH|prior history| h o |h/o',\n",
    "                      row.text[0:100],\n",
    "                      flags=re.I)\n",
    "    nots6 = re.search('(disorder|disease(s)*|abuse|Dysfunction|migraines|anxiety)\\s*\\\n",
    "(mother|father|grandmother|grandfather|brother|sister|maternal|paternal|uncle|aunt|sibling|sibs)',\n",
    "                      row.text,\n",
    "                      flags=re.I)\n",
    "    nots7 = re.search('\\\\byou\\\\b|your\\\\b|\\\\bme know|speak to our|how do|contact us',\n",
    "                      row.text[0:100],\n",
    "                      flags=re.I)\n",
    "    nots8 = re.search('(can|may|might|could) be|(can|that) cause|him or her|\\\n",
    "may (feel|indicate|experience|include)|risk (of|for)|or new|cautions discussed|help prevent|\\\n",
    "(further|additional|any other) symptoms|watch out for|reasons to return',\n",
    "                      row.text[0:100],\n",
    "                      flags=re.I)\n",
    "    nots9 = re.search('\\?',\n",
    "                      row.text[40:100],\n",
    "                      flags=re.I)\n",
    "    nots10 = re.search('\\[include',\n",
    "                      row.text[60:100],\n",
    "                      flags=re.I)\n",
    "    nots11 = re.search('\\\\d \\\\d \\\\d \\\\d \\\\d|score\\s*\\\\d|PROMIS',\n",
    "                      row.text[60:100],\n",
    "                      flags=re.I)\n",
    "    nots12 = re.search('possible',\n",
    "                      row.text[80:100],\n",
    "                      flags=re.I)\n",
    "    nots13 = re.search('clinical protocol designed|side effects which|as safety|me or parents',\n",
    "                      row.text[0:100],\n",
    "                      flags=re.I)\n",
    "    nots14 = re.search('denies any residual',\n",
    "                      row.text[0:100],\n",
    "                      flags=re.I)\n",
    "    nots15 = re.search('without',\n",
    "                      row.text[80:100],\n",
    "                      flags=re.I)\n",
    "    nots = [nots1, nots2, nots3, nots4, nots5, nots6, nots7, nots8, nots9, nots10, nots11, nots12, nots13,\n",
    "           nots14, nots15]\n",
    "    stat = None\n",
    "    for i in nots:\n",
    "        if i != None:\n",
    "            pos = re.search('positive|affirm|yes|feels|has had',\n",
    "                  row.text[80:100],\n",
    "                 flags=re.I)\n",
    "            pos2 = re.search('positive|affirm|yes',\n",
    "                      row.text[100+len(row.tokens):100+len(row.tokens)+5],\n",
    "                     flags=re.I)\n",
    "            pos3 = re.search('\\+',\n",
    "                      row.text[95:105],\n",
    "                     flags=re.I)\n",
    "            if pos == None and pos2 == None and pos3 == None:\n",
    "                stat = \"Absent\"\n",
    "            else:\n",
    "                stat = \"Present\"\n",
    "    if stat == None:\n",
    "         stat = \"Present\"\n",
    "    return stat\n",
    "      \n",
    "terms = {\n",
    "    \"Anosmia/Ageusia\": 'anosmia|paraosmia|dysgeusia|(olfactory dysfun)|\\\n",
    "(sense of smell altered)|(unusual smell)|\\\n",
    "(smell\\\\s*(impairment|loss|defici\\\\w|insensit\\\\w))|\\\n",
    "(absence|loss|diminished|disorder|inability|problem|altered|lack|reduced|lost|distorted|distortion|\\\n",
    "abnormal|sensit\\\\w*)\\\\s*(?:of|with|to)?\\\\s*(?:sense of)?\\\\s*(olfact\\\\w*|smell(?!ing)|taste)|\\\n",
    "((abnormal|change in)\\\\s*(smell|taste))|(smell|taste)\\\\s*alteration',\n",
    "        \"Anxiety\": 'adjustment disorder|(anxiety|anxious)\\\\s?(attack|disorder|state|mood)?|excessive fear|(\\\n",
    "feel[a-z]*|felt)\\\\s(fearful|scared|frightened|jittery|nervous|tense|terrified|uneasy|worried|\\\n",
    "fidgety|panicked|panicky)|nervous(ness| breakdown)|neuro(tic|sis)|panic attack|\\\n",
    "(trouble|difficulty)\\\\s(with)?\\\\srelaxing',\n",
    "         \"Appetite Loss\": \"anorexia(?!\\s*nervosa)|early satiety|(no|small|little|limited|poor)\\s*appetite|\\\n",
    "(lost|losing|loss|lack[a-z]?)\\s*(of)?\\sappetite|\\\n",
    "(doesn't want|not wanting) to eat|(little|no|lack of) interest in (food|eating)\",\n",
    "         \"Chest Pain\": \"chest pain|pain[s]? in [a-z]* chest\",\n",
    "         \"Cough\": '(barking|choking|chronic|painful|non(-)?\\\\s{,3}stop|productive|persistent|paroxysmal|mild|moderate|mod|\\\n",
    "severe|frequent|hacking|irritating|morning|nocturnal|daytime|explosive|dry|spasmodic|wet)?\\\\s{,3}cough(?!assist)(?!response)(s|ing|ed)\\\\s{,3}\\\n",
    "(during|on|after|variant|varient|while|when)?|(frequen[a-z]*)\\\\s{,3}(of)?\\\\s{,3}cough(s|ing)?|\\\n",
    "cough(s|ing|ed)?\\\\s{,3}frequen(cy|tly|t|)?',\n",
    "         \"Dental/Gum Problems\": \"(problem[s]?|issue[s]?|difficult(y|ies)) (in|with) (teeth|gum)|\\\n",
    "(poor|bad) gum|(crack|chip)(s|ed)?\\s*(in)?\\s*teeth|mouth sores|dry mouth(?! liquid)|\\\n",
    "(jaw|tooth) pain|pain in (jaw|tooth|teeth)|(swollen|swelling|bleeding)\\s*(in)?\\s*gums|\\\n",
    "(tooth|teeth|gum[s]?) (problems|issues|difficulty|difficulties)\",\n",
    "         \"Depression\": '(unhappy|depress(ion|ive|ed)(?! right| left| systolic| biventricular| screen| thermodilution)|\\\n",
    "suicid(e|al))|\\\n",
    "((feel(s|ing)|felt)\\\\s*(sad|worthless|helpless|like a failure))|\\\n",
    "(no reason (for|to) liv(e|ing))|(nothing to look forward to)',\n",
    "        \"Digestive Issues\": '(feel[a-z]*|felt)?\\s*nause[a-z]*|vomit[a-z]*|upset stomach|poor appetite|\\\n",
    "throwing up|\\\\bretch[a-z]*|\\\n",
    "(hyper)? emesis|sick to stomach|queasy|\\bpuk[a-z]*|\\bbarf[a-z]*|\\\n",
    "(?<!mitral )(?<!valve )(?<!trivial )(?<!aortic )(?<!flow )(?<!tricuspid )regurgit[a-z]*|loose stool|diarrhea|\\\n",
    "(trouble|difficulty|problems)\\s*(with)?\\s*(defecation|defecating|stooling|pooping)|constipat(ed|ion)|\\\n",
    "(?<!vesicoureteral)(?<!vesicoureteric)(?<!urine) reflux\\\\b',\n",
    "         \"Dizziness\": \"dizz(y|iness)|vertigo|room\\s*(is|was)? spinning\",\n",
    "         \"Excessive Sweating\": \"(excessive|profuse) sweat(ing|s)?|sweat(s|ing) (too much|excessively|profusely)|\\\n",
    "night sweats|hyperhidrosis|diaphoresis\",\n",
    "         \"Excessive Thirst\": \"excess(ive)? thirst|polydipsia\",\n",
    "         \"Fatigue\": '((feel\\\\w*|felt)\\\\s{,3}(exhaust\\\\w*|tired|weak|worn out))|\\\n",
    "((fatigu\\\\w*)|bone-tired|lethargic|listless|malaise|run-down|(general|overall|pervasive|intermittent) weakness|weary|\\\n",
    "sluggish(?! to light)|exhaustion)|((easily|mentally|physically|totally|too)\\\\s{,3}(tired|drained))|(\\\n",
    "(no|not|out of)\\\\s{,3}(?:enough )?energ\\\\w*)|(tire[sd]\\\\s{,3}(quickly|with exercise|all the time))',\n",
    "        \"Fever\": '(?<!hay)fever[a-z]*|pyrexia|(raised|elevated|high)\\\\s*(body)?\\\\s*temp(erature)?|shiver[a-z]*|\\\n",
    "temp(erature)?\\\\s*(spike|raised|elevated|high)|\\\\bfebrile|(feel[a-z]*|felt)\\\\s*hot|rigors',\n",
    "        \"Hair Loss\": \"(loss|shedding) of hair|los(t|ing)\\s*(his|her|my|their)*\\s*hair|\\\n",
    "hair (loss|shedding)|(?<!scar )alopecia|telogen effluvium\",\n",
    "         \"Headache\": \"head\\s*ache[s]?|migraine[s]?(?! team| program| center| psychologist| disability)|\\\n",
    "(?<!goodstart)(?<!extensive)(?<!gerber)(?<!pedsql)(?<!ha to)(?<!reviewed by)(?<!labs due to) \\\\bha\\\\b|\\\n",
    "migrainous\",\n",
    "        \"Heart Problems\": \"(palpitations|tachycardia|heart beat(s|ing)\\s*[a-z]*?\\s*(fast|hard)|\\\n",
    "heart rac(ing|es)|heart pound(s|ing))|syncope|long qt\",\n",
    "        \"Irritability\": \"(?<! while )holding\\s*(his|her|their)?\\s*breath|irritab(ility|le)(?!\\s*bowel)(?!\\s*spot)|\\\n",
    "fuss(ing|y|iness)|crabb(y|iness)|agressivity\",\n",
    "        \"Myalgia\": \"myalgia|muscle (ache|pain)[s]?|muscles hurt|(pain|ache)[s]? in muscle[s]?\",\n",
    "        \"Pain\": '\\\\bpain(?! clinic| center)|stomachache|headache (?! center)|\\\\bach(e|ing)|\\\n",
    "cramp(s|ing)?|sore\\\\b|(?! non-|non )\\\\btender(ness)?',\n",
    "        \"Respiratory Signs & Symptoms\": '(dyspnea)|((difficult[a-z]*|shallow|[^un]labored|rapid)\\\\sbreathing)|\\\n",
    "\\\\bsob\\\\b|(short[a-z]*|\\\\bout\\\\b)\\\\s*(of)?\\\\s*breath|\\\n",
    "(unable|struggl[a-z]*) to breath[e]?|wheezing|(gasping for|runs out of) air|\\\n",
    "winded|tight[a-z]* chest|chest\\\\s*(feels|felt)?\\\\s*tight[a-z]*|restricted airflow|panting|wheezes|\\\n",
    "asthma|\\\\brhonchi|work of breathing',\n",
    "        \"Skin Signs/Symptoms\": \"\\brash|(?<!mouth )(?<!throat )pruritis|\\bitch(y|ing)|\\\n",
    "        skin\\s*[a-z]*\\s*[a-z]*\\s*itch(y|es|ing)?|\\\\brash|\\\\bitch(y|ing)|erythema|swelling\",\n",
    "         \"Cognitive Impairments\": \"brain fog|(attention|concentration|memory|focus) (issues|problems)|\\\n",
    "(difficulty|trouble|problem[s]?|hard(er)? time|issues)\\s*(with)?\\s*(concentrat(ion|ing)|remembering|attention|focus)|\\\n",
    "(decreased|lower)\\s*(ability (with|to)?)?\\s*(attention|concentrat(e|ion)|remember|focus)\\s(?!urine)\",\n",
    "         \"Physical Impairments\": \"(trouble|difficult(y|ies)|problem[s]?|hard time)\\s*[a-z]*\\s*\\\n",
    "((climbing|taking|using)?\\s*stair|walking|running|exercise|(playing )?sports|dressing|getting dressed)|\\\n",
    "functional limitations|postural control\",\n",
    "        \"School Difficulty\": '(miss[a-z]*|drop[a-z]*|fail[a-z]*|difficul[a-z]*)\\s*(in|from|at|with)?\\s*(school|class)|\\\n",
    "(below average|poor|bad|low|failing|falling|worse[a-z]*) grade(s)?(?! temp[a-z]*| fever| glioma| astrocytoma)|\\\n",
    "grade(s)?\\s*(are|have)?\\s*(drop[a-z]*|f[ae]ll[a-z]*|worse[a-z]*)|\\\n",
    "(academic|school|scholastic|education(al)?)\\s*(problem|issue|difficult(y|ie)|trouble|struggle)(s)?|\\\n",
    "(doing (poorly|bad(ly)?)|not doing well|problem|issue|difficult(y|ie)|trouble|struggle|struggling|hard time)\\\n",
    "(s)?\\s*(at|with|in|regarding)\\s*(school|grades|class(es)?)',\n",
    "        \"Sleep Problems\": \"insomnia|(isn't|not) sleeping\\s*((very )?well|enough|much|at all)|\\\n",
    "(trouble|difficult(y|ies)|problems|hard time) sleeping|parasomnia|nightmares|night terrors\"\n",
    "}\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "def to_str(tokens, lower=True):\n",
    "    tokens = eval(tokens)\n",
    "    if lower:\n",
    "        res = \" \".join(tok.lower().strip() for tok in tokens)\n",
    "    else:\n",
    "        res = \" \".join(tok.strip() for tok in tokens)\n",
    "    res = res.replace(\" ,\", \",\")\n",
    "    res = res.replace(\" :\", \":\")\n",
    "    res = res.replace(\" ;\", \";\")\n",
    "    return res\n",
    "\n",
    "save = True\n",
    "\n",
    "for i in terms.keys():\n",
    "    term = {i: re.compile(terms[i], flags=re.I)}\n",
    "    print(i)\n",
    "    res = gen_table_text(term, filter_out, chars=100)\n",
    "    if len(res) > 0:\n",
    "        res[\"regex_assertion\"] = res.apply(newer_regex, axis=1)\n",
    "    # print(res.head())\n",
    "    print(len(res))\n",
    "    if save:\n",
    "        if len(res) > 0:\n",
    "            res.to_csv((f\"s3://{bucket}/{export_name}_subset_\" + i + \".csv\"), index=False)\n",
    "            res = pd.read_csv(f\"s3://{bucket}/{export_name}_subset_\" + i + \".csv\")\n",
    "\n",
    "            res.tokens = res.tokens.apply(to_str)\n",
    "            res.clinical_ner = res.clinical_ner.apply(to_str, args=(False,))\n",
    "            res.wip_ner = res.wip_ner.apply(to_str, args=(False,))\n",
    "            res.BERT_ner = res.BERT_ner.apply(to_str, args=(False,))\n",
    "            res.to_csv((f\"s3://{bucket}/{export_name}_subset_\" + i + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f31449-eb69-4d6a-a48f-bde0009bd6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terms = [\"Anosmia/Ageusia\", \"Anxiety\", \"Appetite Loss\", \"Chest Pain\", \"Cough\",\n",
    "         \"Dental/Gum Problems\", \"Depression\", \"Digestive Issues\", \"Dizziness\", \n",
    "         \"Excessive Sweating\", \"Excessive Thirst\", \"Fatigue\", \"Fever\", \"Hair Loss\", \"Headache\",\n",
    "         \"Heart Problems\",  \"Irritability\", \"Myalgia\", \"Pain\", \"Respiratory Signs & Symptoms\",\n",
    "        \"Skin Signs/Symptoms\", \"Cognitive Impairments\", \"Physical Impairments\",\n",
    "        \"School Difficulty\", \"Sleep Problems\"]\n",
    "\n",
    "conc = []\n",
    "s3 = boto3.client('s3')\n",
    "for i in terms:\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=export_name + \"_subset_\" + i + \".csv\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        pass\n",
    "    else:\n",
    "        df = pd.read_csv(f\"s3://{bucket}/{export_name}_subset_\" + i + \".csv\")\n",
    "        conc.append(df)\n",
    "conc = pd.concat(conc)\n",
    "\n",
    "conc.to_csv((f\"s3://{bucket}/{export_name}_subset_all.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "NLP RECOVER",
   "language": "python",
   "name": "nlprecover"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
