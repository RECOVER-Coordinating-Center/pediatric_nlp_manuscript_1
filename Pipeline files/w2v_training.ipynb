{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21997a8-85a0-4c71-837d-3548936eb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import functools as ft\n",
    "import gc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#if True, save tokens to file\n",
    "#if False (or otherwise), train model using token file\n",
    "get_tokens = True\n",
    "token_filename = \"word2vec/miltoks.txt\"\n",
    "model_filename = \"word2vec/covid_milnotes.model\"\n",
    "\n",
    "if get_tokens:\n",
    "    conn = \"\"\n",
    "    cnx = create_engine(conn)\n",
    "    sites = []\n",
    "\n",
    "    #given a site name (str), return query to get patients who have had covid from that site\n",
    "    def create_query(site):\n",
    "        return \"\"\"with post_covid_pts as (\n",
    "     SELECT person_id, min(observation_date) as covid_index_date\n",
    "     FROM peds_recover.observation_derivation_recover odr\n",
    "     WHERE value_as_concept_name = 'COVID-19 specific diagnosis'\n",
    "     group by person_id\n",
    "    )\n",
    "    SELECT q.person_id new_person_id, q.birth_date, '\"\"\" + site + \"\"\"' as site, covid_index_date, m.*, n.notes\n",
    "    FROM notes.\"\"\" + note_site + \"\"\"_notes n\n",
    "    INNER JOIN notes.\"\"\" + note_site + \"\"\"_metadata m ON n.notes_id = m.notes_id\n",
    "    LEFT JOIN peds_recover.person q on m.person_id = q.site_id\n",
    "    INNER JOIN post_covid_pts pcp on q.person_id = pcp.person_id\"\"\"\n",
    "\n",
    "    dfs = []\n",
    "    for site in sites:\n",
    "      dfs.append(pd.read_sql_query(create_query(site), cnx))\n",
    "    df_covid = pd.concat(dfs)\n",
    "    del dfs\n",
    "\n",
    "    #separate out pre/post\n",
    "    #convert times\n",
    "    df_covid[\"covid_index_date\"] = pd.to_datetime(df_covid[\"covid_index_date\"],\n",
    "                                              format=\"%Y-%m-%d\")\n",
    "    df_covid[\"note_date\"] = pd.to_datetime(df_covid[\"note_date\"],\n",
    "                                          format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    #remove erroneous dates                                    \n",
    "    df_covid = df_covid[df_covid[\"note_date\"] != \"NaT\"]\n",
    "    #blackout period\n",
    "    df_covid[\"timediff\"] = abs((df_covid[\"note_date\"] -\n",
    "                               df_covid[\"covid_index_date\"]).dt.days)\n",
    "    df_covid = df_covid[df_covid[\"timediff\"] > 28]\n",
    "    print(len(df_covid))\n",
    "    gc.collect()\n",
    "\n",
    "#take a df series of text\n",
    "#return text cleaned of unicode code points and bad characters that we have found in the notes\n",
    "def clean_text(text):\n",
    "    text = text.str.replace(\"<82>|<85>\", \",\", regex=True)\n",
    "    text = text.str.replace(\"<92>\", \"'\", regex=False)\n",
    "    text = text.str.replace(\"<93>|<94>\", '\"', regex=True)\n",
    "    #these two seem to mostly (though not always) appear in contexts that should have spaces\n",
    "    text = text.str.replace(\"u00b7u00b0u00b7u00b0\", \" \")\n",
    "    text = text.str.replace(\"u00b7u00b0\", \" \")\n",
    "    #remove raw unicode\n",
    "    text = text.str.replace(\"u0.b.|u2...|u0.a.|<..>\", \"\", regex=True)\n",
    "    #remove other misc. symbols + the strange $& & $!\n",
    "    text = text.str.replace(\"\\$\\&|\\$!|¼|½|·|`|°|•|§|—\", \"\", regex=True)\n",
    "    return text\n",
    "\n",
    "if get_tokens:\n",
    "    df_covid = df_covid.drop_duplicates(subset=\"notes\")\n",
    "    df_covid[\"cleaned_notes\"] = clean_text(df_covid.notes)\n",
    "    df_covid = df_covid.drop_duplicates(subset=\"cleaned_notes\")\n",
    "    print(len(df_covid))\n",
    "    df_covid = df_covid.cleaned_notes\n",
    "    gc.collect()\n",
    "\n",
    "if get_tokens:\n",
    "    seed = 129081\n",
    "    #sample notes (1 million) to reduce tokenization time and memory\n",
    "    df_covid = df_covid.sample(n=1000000, random_state=seed)\n",
    "    gc.collect()\n",
    "\n",
    "#set random seeds for repeatability\n",
    "seed = 129081\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops.add(\"history\")\n",
    "stops.add(\"mg\")\n",
    "stops.add(\"ml\")\n",
    "stops.add(\"md\")\n",
    "stops.add(\"date\")\n",
    "stops.add(\"nd\")\n",
    "stops.add(\"th\")\n",
    "stops.add(\"st\")\n",
    "stops.add(\"rd\")\n",
    "\n",
    "#takes a sentence (str) and a tokenizer\n",
    "#returns a list of tokens\n",
    "def process_text_word2vec(text, tokenizer):\n",
    "    text.replace(\"504\", \"Fiveohfour\")\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    text = re.sub(\"/\", \" \", text)\n",
    "    text = re.sub(\":\", \" \", text)\n",
    "    text = re.sub(\"=\", \" \", text)\n",
    "    remove = \"[\" + string.punctuation + string.digits + \"¿¼½Â·°©®\" + \"]\"\n",
    "    text = re.sub(remove, \"\", text)\n",
    "    #remove multiple spaces\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    #remove common words\n",
    "    tokens = [x for x in tokens if x not in stops]\n",
    "    return tokens\n",
    "\n",
    "#takes note Series and filepath\n",
    "#saves list of tokens to filepath\n",
    "def tokenize(text, filepath):\n",
    "    gc.collect()\n",
    "    sensenlist = []\n",
    "    res = []\n",
    "    for note in text:\n",
    "        senlist = []\n",
    "        note = note.lower()\n",
    "        sens = sent_tokenize(note)\n",
    "        for sen in sens:\n",
    "            #first split again, because many sentences are separated by spaces\n",
    "            s = sen.split(\"  \")\n",
    "            for i in s:\n",
    "                senlist.append(i)\n",
    "        for sen in senlist:\n",
    "            pro = process_text_word2vec(sen, word_tokenize)\n",
    "            if len(pro) > 1:\n",
    "                with open(filepath, encoding=\"utf8\", mode=\"a\") as f:\n",
    "                    for word in pro:\n",
    "                        f.write(word + \" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "#create list of tokens\n",
    "if get_tokens:\n",
    "    tokenize(df_covid, token_filename)\n",
    "    \n",
    "else:\n",
    "    #train model\n",
    "    gc.collect()\n",
    "    sentences = LineSentence(token_filename)\n",
    "    model = Word2Vec(sentences, vector_size=1000,\n",
    "                             workers=4, seed=seed)\n",
    "    model.save(model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
